<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="LLM-Optic: Unveiling the Capabilities of Large Language Models for Universal Visual Grounding">
  <meta name="keywords" content="LLM-Optic, LLM, LMM, Grounding">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LLM-Optic: Unveiling the Capabilities of Large Language Models for Universal Visual Grounding</title>



  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <link rel="icon" href="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7">
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">LLM-Optic: Unveiling the Capabilities of Large Language Models for Universal Visual Grounding</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://haoyu-zhao.github.io/">Haoyu Zhao</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Wenhang Ge</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.yingcong.me/">Ying-Cong Chen</a><sup>1,2</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>The Hong Kong University of Science and Technology (Guangzhou),</span>
            <span class="author-block"><sup>2</sup>The Hong Kong University of Science and Technology</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <img id="overall" width="100%" src="static/images/Comparison.jpg">
              <h2 class="title is-3">Abstract</h2>
              <div class="content has-text-justified">
                  <p>
                    Visual grounding is an essential tool that links user-provided text queries with query-specific regions within an image. Despite advancements in visual grounding models, their ability to comprehend complex queries remains limited. To overcome this limitation, we introduce LLM-Optic, an innovative method that utilizes Large Language Models (LLMs) as an optical lens to enhance existing visual grounding models in comprehending complex text queries involving intricate text structures, multiple objects, or object spatial relationships---situations that current models struggle with. LLM-Optic first employs an LLM as a Text Grounder to interpret complex text queries and accurately identify objects the user intends to locate. Then a pre-trained visual grounding model is used to generate candidate bounding boxes given the refined query by the Text Grounder. After that, LLM-Optic annotates the candidate bounding boxes with numerical marks to establish a connection between text and specific image regions, thereby linking two distinct modalities. Finally, it employs a Large Multimodal Model (LMM) as a Visual Grounder to select the marked candidate objects that best correspond to the original text query. Through LLM-Optic, we have achieved universal visual grounding, which allows for the detection of arbitrary objects specified by arbitrary human language input. Importantly, our method achieves this enhancement without requiring additional training or fine-tuning. Extensive experiments across various challenging benchmarks demonstrate that LLM-Optic achieves state-of-the-art zero-shot visual grounding capabilities.
                  </p>
              </div>
          </div>
      </div>
      <!-- Abstract.-->


      <!-- Model. -->
      <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
              <h2 class="title is-3">Method</h2>
              <div class="content has-text-justified">
                  <p>
                  We propose using LLMs and LMMs as effective reasoning modules for handling complex user queries to achieve universal visual grounding. Our framework includes three key modules: an LLM-based Text Grounder, a Candidate Positioning and Setting Marks module, and an LMM-based Visual Grounder. It does not require any additional training and features a fully modular design, allowing for the seamless integration of rapid advancements in new technologies.
                  </p>
              </div>
              <img id="overall" width="100%" src="static/images/Overview.jpg">
          </div>
      </div>
      <!--/ Model. -->

  </div>
</section>






<section class="section" id="Results">
  <div class="container is-max-desktop content">
      <h2 class="title">Results</h2>
      <div id="main" style="width: 950px; margin: 0 auto;">
          <div class="box" style="text-align: center;">
              <div class="pic">
                  <p><strong>Query</strong>: <em>a giraffe in between two other giraffes.</em></p>
                  <div style="display: flex; justify-content: space-around; align-items: center;">
                      <div style="width: 47%;">
                          <img src="static/images/a giraffe in between two other giraffes1.jpg" style="width: 100%;">
                          <p>(a) Grounding DINO</p>
                      </div>
                      <div style="width: 47%;">
                          <img src="static/images/a giraffe in between two other giraffes.jpg" style="width: 100%;">
                          <p>(b) LLM-Optic</p>
                      </div>
                  </div>
              </div>
          </div>

          <div class="box" style="text-align: center;">
            <div class="pic">
                <p><strong>Query</strong>: <em>a giraffe in between two other giraffes.</em></p>
                <div style="display: flex; justify-content: space-around; align-items: center;">
                    <div style="width: 47%;">
                        <img src="static/images/a guy in a red shirt and jeans on top of a brown horse.jpg" style="width: 100%;">
                        <p>(a) Grounding DINO</p>
                    </div>
                    <div style="width: 47%;">
                        <img src="static/images/a guy in a red shirt and jeans on top of a brown horse_optic.jpg" style="width: 100%;">
                        <p>(b) LLM-Optic</p>
                    </div>
                </div>
            </div>
        </div>

        <div class="box" style="text-align: center;">
          <div class="pic">
              <p><strong>Query</strong>: <em>a giraffe in between two other giraffes.</em></p>
              <div style="display: flex; justify-content: space-around; align-items: center;">
                  <div style="width: 47%;">
                      <img src="static/images/a man wearing red color tshirt pouring wine in glass.jpg" style="width: 100%;">
                      <p>(a) Grounding DINO</p>
                  </div>
                  <div style="width: 47%;">
                      <img src="static/images/a man wearing red color tshirt pouring wine in glass_optic.jpg" style="width: 100%;">
                      <p>(b) LLM-Optic</p>
                  </div>
              </div>
          </div>
      </div>

      <div class="box" style="text-align: center;">
        <div class="pic">
            <p><strong>Query</strong>: <em>a giraffe in between two other giraffes.</em></p>
            <div style="display: flex; justify-content: space-around; align-items: center;">
                <div style="width: 47%;">
                    <img src="static/images/a woman wearing a sweater and shorts pushing a baby stroller1.jpg" style="width: 100%;">
                    <p>(a) Grounding DINO</p>
                </div>
                <div style="width: 47%;">
                    <img src="static/images/a woman wearing a sweater and shorts pushing a baby stroller.jpg" style="width: 100%;">
                    <p>(b) LLM-Optic</p>
                </div>
            </div>
        </div>
    </div>

    <div class="box" style="text-align: center;">
      <div class="pic">
          <p><strong>Query</strong>: <em>a giraffe in between two other giraffes.</em></p>
          <div style="display: flex; justify-content: space-around; align-items: center;">
              <div style="width: 47%;">
                  <img src="static/images/a woman with glasses standing beside a man with glasses1.jpg" style="width: 100%;">
                  <p>(a) Grounding DINO</p>
              </div>
              <div style="width: 47%;">
                  <img src="static/images/a woman with glasses standing beside a man with glasses.jpg" style="width: 100%;">
                  <p>(b) LLM-Optic</p>
              </div>
          </div>
      </div>
  </div>

  <div class="box" style="text-align: center;">
    <div class="pic">
        <p><strong>Query</strong>: <em>a giraffe in between two other giraffes.</em></p>
        <div style="display: flex; justify-content: space-around; align-items: center;">
            <div style="width: 47%;">
                <img src="static/images/far right jet1.jpg" style="width: 100%;">
                <p>(a) Grounding DINO</p>
            </div>
            <div style="width: 47%;">
                <img src="static/images/far right jet.jpg" style="width: 100%;">
                <p>(b) LLM-Optic</p>
            </div>
        </div>
    </div>
</div>
<div class="box" style="text-align: center;">
  <div class="pic">
      <p><strong>Query</strong>: <em>a giraffe in between two other giraffes.</em></p>
      <div style="display: flex; justify-content: space-around; align-items: center;">
          <div style="width: 47%;">
              <img src="static/images/the zebra on the right1.jpg" style="width: 100%;">
              <p>(a) Grounding DINO</p>
          </div>
          <div style="width: 47%;">
              <img src="static/images/the zebra on the right.jpg" style="width: 100%;">
              <p>(b) LLM-Optic</p>
          </div>
      </div>
  </div>
</div>
          <!-- Repeat similar structure for other content if needed -->
      </div>
  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <p>
          If you're using DetGPT in your research or applications, please cite using this BibTeX:
      </p>
      <pre><code>
          @misc{pi2023detgpt,
                title={DetGPT: Detect What You Need via Reasoning},
                author={Renjie Pi and Jiahui Gao and Shizhe Diao and Rui Pan and Hanze Dong and Jipeng Zhang and Lewei Yao and Jianhua Han and Hang Xu and Lingpeng Kong and Tong Zhang},
                year={2023},
                eprint={2305.14167},
                archivePrefix={arXiv},
                primaryClass={cs.CV}
          }
      </code></pre>
  </div>
</section>

<section class="section" id="Acknowledgement">
  <div class="container is-max-desktop content">
      <h2 class="title">Acknowledgement</h2>
      <p>
          This website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed
          under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
          Commons Attribution-ShareAlike 4.0 International License</a>.
      </p>
  </div>
</section>


</body>
</html>